#!/usr/bin/env ruby

###################################################################################
# Usage:
#
# You'll need to setup a scraper.yml file in policy-browser/config/docker before
# running this script if you want to run it on a different consultation from 2015-134.
# Modify at least the "download_folder" to point to a local path on your machine.
###################################################################################

require 'httparty'
require 'nokogiri'
require 'pry'
require 'cgi'
require 'yaml'
require 'mechanize'
require 'fileutils'

require 'optparse'

options = {}
OptionParser.new do |opts|
  opts.on("-c", "--config FILE", "Supply a scraper.yml file for configuration") do |file|
    options[:config] = file
  end
end.parse!

# Return a parse version of the document obtained by making a GET request to the
# provided href.
def parse_get(href)
  text = HTTParty.get(href)
  Nokogiri::HTML(text)
end


# Re-opening the class to add some helper methods
class Nokogiri::HTML::Document
  # Grab all "a" elements that contain the given content text (this is the text a user will see,
  # not necessarily part of the actual link).
  def links_with_text(text)
    self.xpath("//a[contains(text(),'#{text}')]")
  end
  
  # Grab all the 'a' elements from the pagination table. Here we are specifically looking under the first table
  # of the page. Currently this is fine cause the pagination table is the first table, but in future this may change
  # and in such cases we will have to figure out a way to generalize the search.
  def get_all_pages()
    a_links = self.xpath('//tr[1]/td/table/tr/td/a/@href')
    a_links.map {|a_link| a_link.value}
  end
  # Assumption: the document contains a single table with column headers
  #
  # Return all "a" elements in the column represented by the column_header
  #
  # TODO: generalize a bit more to get rid of the "single table in document" assumption
  def links_under_column(column_header)
    # See: https://stackoverflow.com/questions/8015550/find-cells-under-a-column-in-an-html-table-with-xpath

    self.xpath("//tr/td[
      count(preceding-sibling::td[not(@colspan)])
      + sum(preceding-sibling::td/@colspan)
    = count(../../tr[1]/th[.='#{column_header}']/preceding-sibling::th[not(@colspan)])
      + sum(../../tr[1]/th[.='#{column_header}']/preceding-sibling::th/@colspan)]/a")
  end
end

class Nokogiri::XML::Element
  # Assumption: The current element is an "a" element with a link that can be accessed via
  # an HTTP GET request
  #
  # Returns a Nokogiri parsed document of the page referenced by the link
  def follow_link()
    parse_get(self['href'])
  end

  # Assumption: The current element is an "a" element with a link to a downloadable document
  # (accessible via GET request)
  #
  # Downloads the document to the given path.
  def download(path)
    if File.exists?(path)
      puts "#{path} already exists"

      return
    end

    File.open(path, "w") do |file|
      file.binmode
      HTTParty.get(self['href'], stream_body: true) do |fragment|
        file.write(fragment)
      end
    end
  end

  # Assumption: The current element is an "a" element
  #
  # Grabs any query parameters and returns them in a hash
  def link_params
    CGI::parse(URI.parse(self['href']).query)
  end
end

def download_document(link_params, down_link)
  
    # Guess the type of file and extract enough information to give it a unique name
    # among all the files (in a flat folder structure)

    header = HTTParty.head(link_params['href'])
    filename_hint = header['content-disposition'] 
    # The naming convention used here is a bit different than the one used for Interventions and Replies
    # Some documents have only a "Key" which I believe is the 'ID' and do not have the DMID
    # So we will use the Key as id followed by the consultation number and Type of the document.

    # Type - currently grabbing the name of the document as listed on the page. We can change this
    # to something more useful later.

    # TODO: See the mirror comment to this in scripts/wrangling/process-docs.py. If you're looking
    # at this section because you need to add another piece of information to the filename in order
    # to keep track of meta-information we'll eventually want to import into the database, strongly
    # consider implementing a sha256 hash function to match the python version, using that for the
    # filename, and keeping track of the rest of the information in a json file (with a basename that
    # is the sha256 hash of the original file's contents). Right now, process-docs.py is creating
    # these .json files from the filename information generated here, but it is an extra step that
    # really wouldn't be needed if we just generated the json files here.
    meta = [link_params['en']] +[link_params['ID']] + [link_params['DMID']] + [link_params['type']]
    filename = meta.join(".")
    extension = "html"
    if filename_hint =~ /filename=(.*?)\.(.+)$/i
      # Not all files have a filename, so lets just use unknown for now
      filename += "(unknown)"
      extension = $2
    end
    
    # Do the actual download
    download_path = File.join(DOWNLOAD_FOLDER, "#{filename}.#{extension.downcase}")
    down_link.download(download_path)
  
end


def get_related_documents(intervention_page, doc_type)
    
    related_documents = intervention_page.links_with_text("Related Documents")
    # for testing (restrict the loop to a 1 element array, containing the first of the returned results):
    # related_documents = [related_documents.first]
    
    for related_document in related_documents do
      related_document_page = related_document.follow_link
      download_links = related_document_page.links_under_column("Document Name")
      for download_link in download_links do
        # Guess the type of file and extract enough information to give it a unique name
        # among all the files (in a flat folder structure)
        header = HTTParty.head(download_link['href'])
        filename_hint = header['content-disposition']
        extension = "unknown"
        #meta =  related_document.link_params['en'] + related_document.link_params['ID'] + download_link.link_params['DMID'] + [doc_type]
        meta =  related_document.link_params['en'] + related_document.link_params['ID'] + download_link.link_params['DMID'] + [download_link.text]
        filename = meta.join(".")
        if filename_hint =~ /filename=(.*?)\.(.+)$/i
          # The filenames seem pretty random and are unlikely to be useful, but just in case...
          filename += "(#{$1})"
          extension = $2
        end
        
        # Do the actual download
        download_path = File.join(DOWNLOAD_FOLDER, "#{filename}.#{extension.downcase}")
        download_link.download(download_path)
      end
    end
  
  
end

# Figure out where this script is running and find the config file relative to it
script_path = File.expand_path(File.dirname(__FILE__))
config_path = config_path = options[:config] || File.join(script_path,"..","..","scraper.yml")

# Load the config file
scraper_config = YAML.load_file(config_path)

# Grab config entries for the URL to start at and the folder to download to
entry_url = scraper_config['entry_url']
DOWNLOAD_FOLDER = scraper_config['download_folder']
#REGEX = /javascript:__doPostBack\('ctl00$MainContent$gvData','(Page\$\d+)'\)/
TARGET = 'ctl00$MainContent$gvData'
consultation_status = scraper_config['ongoing_consultation']
consultation = scraper_config['consultation']
consultation_nohyphen = consultation.gsub('-', '')


# Look for the first table in the entry page and get the contents of all the rows,
# then iterate through them to get the different consultations and its Related documents

entry_page = parse_get(entry_url)
table = entry_page.css('table.item').first
rows = table.css('tr')
text_all_rows = Hash.new
rows.each do |row|
    # we get the notice number from first row
    if consultation_status.to_s.include?("No")
      row_name = row.css('td[1]/a').text.match(/\d{4}+-\d{3}\b+|\d{4}+-\d{2}\b+/)
    elsif consultation_status.to_s.include?("Yes")
      #for ongoing consultations, simply grab the first column and change filter to work with this further below
      row_name = row.css('td[1]/a')
    end

    # We get all the hrefs and texts of each individual consultation row (<td>)
    row_href_values = row.css('td[5]/a')
    row_text_values = row.css('td[5]/a').map(&:text)
 
    # We map the consultation number (key), to all the links and corresponding 
    # document type (text) associated with it.
    if row_name.to_s!=""
      text_all_rows[row_name.to_s] = [row_text_values, row_href_values]
    end
    
end

#If the scraper stops on a certain page of interventions, can change this number in order to resume
#where it left off.
start_page = 0

text_all_rows.each do |key, value|
  if !key.nil? && !value.nil? && key.include?(consultation) || key.include?(consultation_nohyphen)
    key = consultation
    links = value[1]
    types = value[0]
    
     for val in 0..links.length-1
      link_params = Hash.new
      link = links[val]
      type = types[val]
      link_params['en'] = key

      if link.to_s.include? "OpenDocument.aspx"
        
        link_parameters = CGI::parse(URI.parse(link['href']).query)
        link_params['type'] = types[val]
        
        link_params['href'] = link['href']
        link_params['ID'] = link_parameters["Key"][0]
        link_params['DMID'] = 1000001
        linktype = link_params['type']
        print("Downloading #{linktype}\n")
        download_document(link_params, link)
      elsif link.to_s.include? "ListeInterventionList"
          
          print("Downloading #{type}\n")
          intervention_page_one = link.follow_link
          #For links including ListeInterventionList, grab the number of results and calculate the total number of pages
          #to download from; without this, the scraper would only grab the first 7 pages (even though there were 151 pages
          #for consultation 2018-0046-7 at the time of writing)
          total_results = intervention_page_one.xpath('//span[@id="ctl00_MainContent_lblRecords"]').text
          total_num = total_results.match(/\d+/)
          total_pages = (total_num.to_s.to_f / 50).to_i + 1

          # Grab related documents in the page one first and then move on to scraping all the other pages
          ##get_related_documents(intervention_page_one, type)
          # Get the event number to construct the url link to Page$1 of an intervention. 
          event_no = link.link_params['en'][0]
          request_url = link['href']
          agent = Mechanize.new()
          page = agent.get(request_url)
          
          form = page.form_with(:id => 'aspnetForm')
          max_page_val_visited = 0
          page_links = intervention_page_one.get_all_pages()
          
          # Get related documents by loading each page.
          #while (page_val = page_links.shift)
          for p_num in start_page..total_pages
            num = "Page$#{p_num}"
            p_num = num.match(/\d+|Last+/)[0].to_s
            if (max_page_val_visited < p_num.to_i) 
              max_page_val_visited = p_num.to_i==0 ? max_page_val_visited+1 : p_num.to_i
              
              form['__EVENTTARGET'] = TARGET
              
              form['__EVENTARGUMENT'] = num
              my_page = form.submit
              
              sleep 1
        
              # Convert Mechanize Page to Nokogiri html page
              my_html = Nokogiri::HTML(my_page.parser.to_html)
              
              get_related_documents(my_html, type)
        
            end
          end
        else
          link_params['type'] = types[val]
          link_params['href'] = link['href']
          # Some Related documents are just html links, in such case they do not
          # have both the case number and document id, so to maintain a decent naming convention, lets 
          # just give them all a dummy 6-digit case number i.e., 100001 and 7-digit DMID 10000001
          # CHANGE THIS LATER
          link_params['ID'] = 100001
          link_params['DMID'] = 1000001
          linktype = link_params['type']
          print("Downloading #{linktype}\n")
          download_document(link_params, link)       
      end

     end
  end

end


