#!/usr/bin/env ruby

###################################################################################
# Usage:
#
# You'll need to setup a scraper.yml file in the project's root folder before
# running this script. Copy ../../scraper.yml.example to ../../scraper.yml and
# modify at least the "download_folder" to point to a local path on your machine.
###################################################################################

require 'HTTParty'
require 'Nokogiri'
require 'Pry'
require 'cgi'
require 'yaml'
require 'mechanize'
require 'fileutils'

# Return a parse version of the document obtained by making a GET request to the
# provided href.
def parse_get(href)
  text = HTTParty.get(href)
  Nokogiri::HTML(text)
end


# Re-opening the class to add some helper methods
class Nokogiri::HTML::Document
  # Grab all "a" elements that contain the given content text (this is the text a user will see,
  # not necessarily part of the actual link).
  def links_with_text(text)
    self.xpath("//a[contains(text(),'#{text}')]")
  end
  
  # Grab all the 'a' elements from the pagination table. Here we are specifically looking under the first table
  # of the page. Currently this is fine cause the pagination table is the first table, but in future this may change
  # and in such cases we will have to figure out a way to generalize the search.
  def get_all_pages()
    a_links = self.xpath('//tr[1]/td/table/tr/td/a/@href')
    a_links.map {|a_link| a_link.value}
  end
  # Assumption: the document contains a single table with column headers
  #
  # Return all "a" elements in the column represented by the column_header
  #
  # TODO: generalize a bit more to get rid of the "single table in document" assumption
  def links_under_column(column_header)
    # See: https://stackoverflow.com/questions/8015550/find-cells-under-a-column-in-an-html-table-with-xpath

    self.xpath("//tr/td[
      count(preceding-sibling::td[not(@colspan)])
      + sum(preceding-sibling::td/@colspan)
    = count(../../tr[1]/th[.='#{column_header}']/preceding-sibling::th[not(@colspan)])
      + sum(../../tr[1]/th[.='#{column_header}']/preceding-sibling::th/@colspan)]/a")
  end
end

class Nokogiri::XML::Element
  # Assumption: The current element is an "a" element with a link that can be accessed via
  # an HTTP GET request
  #
  # Returns a Nokogiri parsed document of the page referenced by the link
  def follow_link()
    parse_get(self['href'])
  end

  # Assumption: The current element is an "a" element with a link to a downloadable document
  # (accessible via GET request)
  #
  # Downloads the document to the given path.
  def download(path)
    if File.exists?(path)
      puts "#{path} already exists"

      return
    end

    File.open(path, "w") do |file|
      file.binmode
      HTTParty.get(self['href'], stream_body: true) do |fragment|
        file.write(fragment)
      end
    end
  end

  # Assumption: The current element is an "a" element
  #
  # Grabs any query parameters and returns them in a hash
  def link_params
    CGI::parse(URI.parse(self['href']).query)
  end
end

def get_related_documents(intervention_page)
  
    related_documents = intervention_page.links_with_text("Related Documents")
    # for testing (restrict the loop to a 1 element array, containing the first of the returned results):
    # related_documents = [related_documents.first]
    
    for related_document in related_documents do
      related_document_page = related_document.follow_link
      download_links = related_document_page.links_under_column("Document Name")
      for download_link in download_links do
        # Guess the type of file and extract enough information to give it a unique name
        # among all the files (in a flat folder structure)
        header = HTTParty.head(download_link['href'])
        filename_hint = header['content-disposition']
        extension = "unknown"
        meta = related_document.link_params['ID'] + related_document.link_params['en'] + download_link.link_params['DMID']
        filename = meta.join(".")
        if filename_hint =~ /filename=(.*?)\.(.+)$/i
          # The filenames seem pretty random and are unlikely to be useful, but just in case...
          filename += "(#{$1})"
          extension = $2
        end
        
        # Do the actual download
        download_path = File.join(DOWNLOAD_FOLDER, "#{filename}.#{extension.downcase}")
        download_link.download(download_path)
      end
    end
  
  
end

# Figure out where this script is running and find the config file relative to it
script_path = File.expand_path(File.dirname(__FILE__))
config_path = File.join(script_path,"..","..","scraper.yml")

# Load the config file
scraper_config = YAML.load_file(config_path)

# Grab config entries for the URL to start at and the folder to download to
entry_url = scraper_config['entry_url']
DOWNLOAD_FOLDER = scraper_config['download_folder']
REGEX = /javascript:__doPostBack\('ctl00$MainContent$gvData','(Page\$\d+)'\)/
TARGET = 'ctl00$MainContent$gvData'

entry_page = parse_get(entry_url)

# Grab only links on the entry page that display "Interventions" as their link text
interventions = entry_page.links_with_text("Interventions")

# for testing (restrict the loop to a 1 element array, containing the first of the returned results):
# interventions = [interventions.first]
for intervention in interventions do
  intervention_page_one = intervention.follow_link
  
  # Grab related documents in the page one first and then move on to scraping all the other pages
  get_related_documents(intervention_page_one)
  # Get the event number to construct the url link to Page$1 of an intervention. 
  event_no = intervention.link_params['en'][0]  
  request_url = "https://services.crtc.gc.ca/pub/ListeInterventionList/Default-Defaut.aspx?en=#{event_no}&dt=i&lang=e"
  agent = Mechanize.new()
  page = agent.get(request_url)
  
  form = page.form_with(:id => 'aspnetForm')
  # edge case: the first page of results
  max_page_val_visited = 1
  page_links = intervention_page_one.get_all_pages()
  # Get related documents by loading each page.
  while (page_val = page_links.shift)
    num = page_val.match(/(Page\$\d|Page\$Last+)/).to_s
    p_num = num.match(/\d|Last+/)[0].to_s
    if (max_page_val_visited < p_num.to_i) || (p_num.match("Last"))
      max_page_val_visited = p_num.to_i==0 ? max_page_val_visited+1 : p_num.to_i
      form['__EVENTTARGET'] = TARGET
      form['__EVENTARGUMENT'] = num
      my_page = form.submit
      sleep 1

      # Convert Mechanize Page to Nokogiri html page
      my_html = Nokogiri::HTML(my_page.parser.to_html)

      get_related_documents(my_html)

    end
  end
end

# Uncomment for console-based debugging
# Pry.start(binding)